xlab("Year") +
ylab("Precipitation")
#Generate the region wise plot
merged3  %>%
ggplot()+
geom_smooth(aes(year, abs(5*central_wtem_diff)))+
geom_smooth(aes(year, abs(central_wpre_diff)), color = 'yellow')+
geom_smooth(aes(year, central_growth_agrgdpl), color = 'red')+
geom_smooth(aes(year,central_growth_rgdpl+5), color = 'navy')+
#geom_smooth(aes(year, cenrtal_growth_indgdpl+5), color= 'black')+
#geom_smooth(aes(year, ki), color = 'pink')+
facet_wrap(~region)+
labs(x = 'Year',
y = 'Change in Temp, Rainfall, Agriculture, GDP',
title = 'Impact of Climate Change on Agriculture - Changes in Temperature & Rainfall')
ggsave("plot2.pdf")
qplot(mean_wtem, mean_polity, colour = poor, data= climate_growth, main="Temperature and Polity Average")  +
xlab("Average Annual Temperature (C)") +
ylab("Polity Average") +
labs(fill = "Poor")
#Average temperature over against leader irregular
qplot(mean_wtem, sum_leader_irregular, colour = poor, data= climate_growth, main="Temperature  and Irregular Leadership Transitions")  +
xlab("Average Annual Temperature (C)") +
ylab("Total of leader irregular") +
labs(fill = "Poor")
qplot(mean_wtem, mean_Value, main="Temperature and Political Stability: 1996-2006",
color = poor,
shape = poor,
data= growthWB) +
xlab("Avg temp") +
ylab("Pol. Stability and Absence of Violence/Terrorism")
#Polity and Temp
qplot(mean_wtem, GPPppMean, main="Average Polity and GDP/PC",
color = poor,
size = mean_polity,
xlim = c(0,30),
ylim = c(0,20000),
data= mergeV3)  +
xlab("Average Annual Temperature (C)") +
ylab("Average GDP/PC")
#Polity and Temp Plot, only Africa
qplot(mean_wtem, GPPppMean, main="Temperature and GDP/PC - Africa",
size = mean_polity,
color = poor,
shape = poor,
xlim = c(0,30),
ylim = c(0,10000),
data= mergeV4_Africa) +
xlab("Average Annual Temperature (C)") +
ylab("Average GGDP/PC")
pbirthday(4)
pbirthday(13)
pbirthday(23)
pbirthday(30)
pnorm(25, mean = 13.9, sd = 2.67)
pnorm(25, mean = 13.9, sd = 2.67, lower.tail = T)
pnorm(25, mean = 13.9, sd = 2.67, lower.tail = F)
1-pnorm(25, mean = 13.9, sd = 2.67)
2.67*2
2.67*2+13.9
1.96*2.67+13.9
pnorm(1)
pnorm(1,lower.tail = F)
pnorm(1)-pnorm(-1)
pnorm(1.96)
pnorm(1.96)-pnorm(-1.96)
qnorm(0.975)
choose(11000,5)
choose(11000,100)
samp_size <- 116
# Calculate the standard error of the null distribution
# assuming an equal probability for each digit
se_null <- sqrt(0.1*0.9/samp_size)
# Set the number of simulations
n_sim <- 10^4
# Specify your alpha level. Given the null is true
# we would hope to falsely reject the null in only alpha percent of
# our samples.
alpha_pct <- 0.05
# Calculate the critical value of the normal distribution which is used
# to decide we reject or accept the null hypothesis
z_crit <- qnorm(1-alpha_pct/2)
reject_holder_pct <- c()
reject_holder_chisq <- c()
for (i in 1:n_sim) {
samp_loop <- sample(0:9, size = samp_size, replace = T)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
if (z_stat >= z_crit) {
reject_holder_pct[i] <- 1
} else{
reject_holder_pct[i] <- 0
}
# chi-square Test
chisq_loop <- chisq.test(table(samp_loop))
reject_holder_chisq[i] <- (chisq_loop$p.value<=alpha_pct)
}
# In this simulation we assumed no bias in the sampling.
# The mean of the reject_holder_pct in this simulations
# measures the false rejection rate (Type I error)
mean(reject_holder_pct)
# We can see from the simulation that by selecting the digit with the
# greatest deviation ex-post, our false rejection rate is well above
# the 5% specified by our alpha level. Here it is close to 50%!
# Contrast this to the chi-square test, which rejects
# this null hypothesis only 5% of the time, which agrees with
# what we would expect given the alpha level we set
mean(reject_holder_chisq)
# Calculate the critical value of the normal distribution which is used
# to decide we reject or accept the null hypothesis
z_crit <- qnorm(1-alpha_pct/2)
z_crit
reject_holder_pct <- c()
reject_holder_chisq <- c()
samp_loop <- sample(0:9, size = samp_size, replace = T)
samp_loop
table(samp_loop)
table(samp_loop)-samp_size*0.1
abs(table(samp_loop)-samp_size*0.1)
max(abs(table(samp_loop)-samp_size*0.1))
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
max_diff
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
max_diff/samp_size
chisq_loop <- chisq.test(table(samp_loop))
chisq_loop
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
samp_loop <- sample(0:9, size = samp_size, replace = T)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
max_diff
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
samp_loop <- sample(0:9, size = samp_size, replace = T)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
samp_loop <- sample(0:9, size = samp_size, replace = T)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
samp_loop <- sample(0:9, size = samp_size, replace = T)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
samp_loop <- sample(0:9, size = samp_size, replace = T)
samp_loop
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
max_diff
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
table(samp_loop)
samp_loop <- sample(0:9, size = samp_size, replace = T)
samp_loop
table(samp_loop)
samp_loop <- sample(0:9, size = samp_size, replace = T)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
max_diff
samp_loop <- sample(0:9, size = samp_size, replace = T)
max_diff
samp_loop <- sample(0:9, size = samp_size, replace = T)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
max_diff
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
samp_size <- 116
# Calculate the standard error of the null distribution
# assuming an equal probability for each digit
se_null <- sqrt(0.1*0.9/samp_size)
# Set the number of simulations
n_sim <- 10^4
# Specify your alpha level. Given the null is true
# we would hope to falsely reject the null in only alpha percent of
# our samples.
alpha_pct <- 0.05
# Calculate the critical value of the normal distribution which is used
# to decide we reject or accept the null hypothesis
z_crit <- qnorm(1-alpha_pct/2)
reject_holder_pct <- c()
reject_holder_chisq <- c()
for (i in 1:n_sim) {
samp_loop <- sample(0:9, size = samp_size, replace = T)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
if (z_stat >= z_crit) {
reject_holder_pct[i] <- 1
} else{
reject_holder_pct[i] <- 0
}
# chi-square Test
chisq_loop <- chisq.test(table(samp_loop))
reject_holder_chisq[i] <- (chisq_loop$p.value<=alpha_pct)
}
# In this simulation we assumed no bias in the sampling.
# The mean of the reject_holder_pct in this simulations
# measures the false rejection rate (Type I error)
mean(reject_holder_pct)
# We can see from the simulation that by selecting the digit with the
# greatest deviation ex-post, our false rejection rate is well above
# the 5% specified by our alpha level. Here it is close to 50%!
# Contrast this to the chi-square test, which rejects
# this null hypothesis only 5% of the time, which agrees with
# what we would expect given the alpha level we set
mean(reject_holder_chisq)
samp_loop <- sample(0:9, size = samp_size, replace = T)
samp_loop
table(samp_loop)
max(abs(table(samp_loop)-samp_size*0.1))
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
samp_loop <- sample(0:9, size = samp_size, replace = T)
table(samp_loop)
samp_loop <- sample(0:9, size = samp_size, replace = T)
table(samp_loop)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
max_diff
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
samp_loop <- sample(0:9, size = samp_size, replace = T)
table(samp_loop)
samp_loop <- sample(0:9, size = samp_size, replace = T)
table(samp_loop)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
max_diff
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
samp_loop <- sample(0:9, size = samp_size, replace = T)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
max_diff
samp_loop <- sample(0:9, size = samp_size, replace = T)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
max_diff
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
samp_loop <- sample(0:9, size = samp_size, replace = T)
# Extract the largest deviation from the expected value
max_diff <- max(abs(table(samp_loop)-samp_size*0.1))
# test if this difference is significant
z_stat <- (max_diff/samp_size)/se_null
z_stat
max_diff
library(tidyverse)
library(readxl)
set.seed(1)
rm(list = ls())
getwd()
library(tidyverse)
library(glmnet)
library(rsample)
setwd("C:/Users/Casey/Dropbox/API-209 - 2020/Problem Set/PS 10/4 - to post - ps 10")
df<-read_csv("osha.csv")
class_df<-sapply(colnames(df), function(x) class(df[[x]]))
View(class_df)
#OLS
f_short_c <- injury_rate ~ has_tmin1_odi + any_insp_prior +
any_complaint_tmin13 + num_nonfat_comp_insp_cy_tc99mm1 +
initial_pen_cy_mzmm1 + ln_initial_pen_cy_mzmm1 +
dafw_analysis_rec_tc99mm1
lm1<-lm(f_short_c, df)
summary(lm1)
#KITCHEN-SINK OLS
rm_vars <- c("sst_year",
"estab_id_duns",
"injuries",
"injury_rate",
"high_injury_rate")
long_vars  <- setdiff(colnames(df), rm_vars)
f_long_rhs <- str_c(long_vars, collapse = " + ")
f_long_c   <- as.formula(str_c("injury_rate ~ ", f_long_rhs))
lm2<-lm(f_long_c, df)
summary(lm2)
#SPLITTING THE DATA SET INTO TRAINING AND TEST
set.seed(02138)
test_ids        <- sample(seq(nrow(df)),
round(0.3 * nrow(df)))
training_ids    <- which(!(seq(nrow(df)) %in% test_ids))
test_data       <- df[test_ids,]
training_data   <- df[training_ids,]
#LASSO ON THE TRAINING SET
y_c_train <- training_data$injury_rate
x_train   <- as.matrix(select(training_data, !!!long_vars))
lasso_1<-glmnet(x_train, y_c_train)
lasso_1
lasso_1<-cv.glmnet(x_train, y_c_train)
lasso_1
print(lasso_1)
#coef(lasso_1, s =  LAMBDA)
#PREDICTION ON THE TRAINING SET
#OLS
lm1$coefficients
summary(lm1)
predict_lm1_train  <- predict(lm1,  select(training_data, c(has_tmin1_odi, any_insp_prior,
any_complaint_tmin13, num_nonfat_comp_insp_cy_tc99mm1,
initial_pen_cy_mzmm1, ln_initial_pen_cy_mzmm1,
dafw_analysis_rec_tc99mm1)))
MSE_lm1_train <-mean((predict_lm1_train - y_c_train)^2)
MSE_lm1_train
print(round(MSE_lm1_train,4))
#KITCHEN-SINK OLS
lm2$coefficients
summary(lm2)
predict_lm2_train  <- predict(lm2, select(training_data, !!!long_vars))
MSE_lm2_train <-mean((predict_lm2_train - y_c_train)^2)
print(round(MSE_lm2_train,4))
#LASSO WITH 10-FOLD CROSS VALIDATION *for each of the 100 lambdas, the algorithm performs a 10-fold cross validation on the data)
lasso_train     <- cv.glmnet(x = x_train,
y = y_c_train)
plot(lasso_train)
#########################
#3 MODELS ON THE TEST SET
y_c_test <- test_data$injury_rate
x_test   <- as.matrix(select(test_data, !!!long_vars))
#OLS
predict_lm1_test  <- predict(lm1,  select(test_data, c(has_tmin1_odi, any_insp_prior,
any_complaint_tmin13, num_nonfat_comp_insp_cy_tc99mm1,
initial_pen_cy_mzmm1, ln_initial_pen_cy_mzmm1,
dafw_analysis_rec_tc99mm1)))
MSE_lm1_test <-mean((predict_lm1_test - y_c_test)^2)
MSE_lm1_test
print(round(MSE_lm1_test,4))
#KITCHEN-SINK OLS
predict_lm2_test  <- predict(lm2, select(test_data, !!!long_vars))
MSE_lm2_test <-mean((predict_lm2_test - y_c_test)^2)
print(round(MSE_lm2_test,4))
#LASSO WITH 10-FOLD CROSS VALIDATION ON TEST SET
predict_lasso_test<-predict(lasso_train, x_test, s = "lambda.min")
colnames(predict_lasso_test)<-c("predict_lasso_test")
MSE_lasso_test <-mean((predict_lasso_test - y_c_test)^2)
print(round(MSE_lasso_test ,4))
print(round(MSE_lm2_test,4))
###CLASSIFICATION PROBLEM
y_b_train <- training_data$high_injury_rate
f_short_b <- update(f_short_c, high_injury_rate ~ .)
f_long_b  <- update(f_long_c, high_injury_rate ~ .)
logistic_short     <- glm(f_short_b,
family = binomial,
data = training_data)
logistic_short
logistic_short_alter     <- glm(f_short_b,
family = "binomial",
data = training_data)
logistic_short_alter
summary(logistic_short)
logistic_short_predict      <- predict(logistic_short,
test_data,
type = "response")
logistic_short_predict
head(logistic_short_predict)
logistic_accuracy_short     <- mean(class_predictions_short == test_data$high_injury_rate)
print(logistic_accuracy_short)
logistic_short_predict      <- predict(logistic_short,
test_data,
type = "response")
head(logistic_short_predict)
class_predictions_short     <- as.numeric(logistic_short_predict > 0.5)
logistic_accuracy_short     <- mean(class_predictions_short == test_data$high_injury_rate)
logistic_accuracy_short
print(logistic_accuracy_short)
true_positives_short <- sum(class_predictions_short == 1 & test_data$high_injury_rate == 1)
all_positives_short<-sum(test_data$high_injury_rate == 1)
false_positives_short<-sum(class_predictions_short == 1 & test_data$high_injury_rate == 0)
precision_short<-print(round(true_positives_short/(false_positives_short+true_positives_short),4))
false_negatives_short <- sum(class_predictions_short == 0 & test_data$high_injury_rate == 1)
recall_short<-print(round(true_positives_short/(true_positives_short+false_negatives_short),4))
logistic_long    <- glm(f_long_b,
family = binomial,
data = training_data)
summary(logistic_long )
logistic_long_predict  <- predict(logistic_long,
test_data,
type = "response")
head(logistic_long_predict)
class_predictions_long     <- as.numeric(logistic_long_predict > 0.5)
logistic_accuracy_long     <- mean(class_predictions_long == test_data$high_injury_rate)
print(logistic_accuracy_long)
true_positives_long <- sum(class_predictions_long == 1 & test_data$high_injury_rate == 1)
all_positives_long<-sum(test_data$high_injury_rate == 1)
false_positives_long<-sum(class_predictions_long == 1 & test_data$high_injury_rate == 0)
precision_long<-print(round(true_positives_long/(false_positives_long+true_positives_long),4))
false_negatives_long <- sum(class_predictions_long == 0 & test_data$high_injury_rate == 1)
recall_long<-print(round(true_positives_long/(true_positives_long+false_negatives_long),4))
#LASSO 10-K FOLD CROSS-VALIDATION
lasso_b     <- cv.glmnet(x = x_train,
y = y_b_train, family = binomial)
#LASSO 10-K FOLD CROSS-VALIDATION
lasso_b     <- cv.glmnet(x = x_train,
y = y_b_train, family = "binomial")
print(lasso_b$lambda)
plot(lasso_b)
#Cross Validation results (averages error of the 10 fold cross validation for each lambda )
print(round(lasso_b$cvm,4))
print(lasso_b$lambda.min)
print(round(lasso_b$cvm,4)[which.min(lasso_b$cvm)])
predict_lasso_b <-predict(lasso_b, x_test, s = "lambda.min")
colnames(predict_lasso_b)<-c("predict_lasso_b")
class_predictions_lasso     <- as.numeric(predict_lasso_b > 0.5)
accuracy_lasso     <- mean(class_predictions_lasso == test_data$high_injury_rate)
library(tidyverse)
library(rsample)
library(glmnet)
setwd("C:/Users/Casey/Dropbox/API-209 - 2020/Problem Set/PS 10/4 - to post - ps 10")
osha <- read_csv("osha.csv")
#1 A and 4(i)
f_short_c <- injury_rate ~ has_tmin1_odi + any_insp_prior +
any_complaint_tmin13 + num_nonfat_comp_insp_cy_tc99mm1 +
initial_pen_cy_mzmm1 + ln_initial_pen_cy_mzmm1 +
dafw_analysis_rec_tc99mm1
injury_rate_short <- lm(f_short_c, data = osha)
#1 B and 4(ii)
rm_vars <- c("sst_year",
"estab_id_duns",
"injuries",
"injury_rate",
"high_injury_rate")
long_vars <- setdiff(colnames(osha), rm_vars)
f_long_rhs <- str_c(long_vars, collapse = " + ")
f_long_c <- as.formula(str_c("injury_rate ~ ", f_long_rhs))
injury_rate_long <- lm(f_long_c, data = osha)
set.seed(02138)
split_osha <- initial_split(osha, prop = 0.7)
osha_train <- training(split_osha)
osha_test <- testing(split_osha)
y_c_train <- osha_train$injury_rate
X_train <- as.matrix(select(osha_train, !!!long_vars))
lasso_osha <- cv.glmnet(x= X_train, y = y_c_train)
lasso_osha$lambda.min
plot(lasso_osha)
coefs_lasso <- enframe(coef(lasso_osha, s = 'lambda.min')[,1])
X_test <- model.matrix(injury_rate ~ ., data = osha_test )[,-1]
osha_test %>%
mutate(
pred_lasso = as.vector(predict(lasso_osha, newx = X_test, s= "lambda.min"))
)
pred_lasso = as.vector(predict(lasso_osha, newx = X_test, s= "lambda.min"))
X_test  <- as.matrix(select(osha_test, !!!long_vars))
pred_lasso = as.vector(predict(lasso_osha, newx = X_test, s= "lambda.min"))
library(tidyverse)
library(rsample)
library(glmnet)
osha <- read_csv("osha.csv")
#1 A and 4(i)
f_short_c <- injury_rate ~ has_tmin1_odi + any_insp_prior +
any_complaint_tmin13 + num_nonfat_comp_insp_cy_tc99mm1 +
initial_pen_cy_mzmm1 + ln_initial_pen_cy_mzmm1 +
dafw_analysis_rec_tc99mm1
injury_rate_short <- lm(f_short_c, data = osha)
#1 B and 4(ii)
rm_vars <- c("sst_year",
"estab_id_duns",
"injuries",
"injury_rate",
"high_injury_rate")
long_vars <- setdiff(colnames(osha), rm_vars)
f_long_rhs <- str_c(long_vars, collapse = " + ")
f_long_c <- as.formula(str_c("injury_rate ~ ", f_long_rhs))
injury_rate_long <- lm(f_long_c, data = osha)
set.seed(02138)
split_osha <- initial_split(osha, prop = 0.7)
osha_train <- training(split_osha)
osha_test <- testing(split_osha)
y_c_train <- osha_train$injury_rate
X_train <- as.matrix(select(osha_train, !!!long_vars))
lasso_osha <- cv.glmnet(x= X_train, y = y_c_train)
lasso_osha$lambda.min
plot(lasso_osha)
coefs_lasso <- enframe(coef(lasso_osha, s = 'lambda.min')[,1])
X_test  <- as.matrix(select(osha_test, !!!long_vars))
pred_lasso = as.vector(predict(lasso_osha, newx = X_test, s= "lambda.min"))
pred_lasso
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(options(width = 60))
knitr::opts_chunk$set(class.output = "bg-warning")
packages <- c('haven','dplyr', 'ggplot2', 'reshape2', 'tidyverse', 'pracma',
'lubridate', 'scales', 'ggthemes', 'gt', 'tidymodels', 'flextable',
'rsample', 'knitr', 'hdm', 'pROC', 'glmnet', 'randomForest')
to_install <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(to_install)>0) install.packages(to_install,
repos='http://cran.us.r-project.org')
lapply(packages, require, character.only=TRUE)
rm(list = ls())
setwd("~/Dropbox/Harvard/03_Fall_2022/API-209 - Advanced Quantitative Method I/Homework/Problem Set 10 - Due 221108")
